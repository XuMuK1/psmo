{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8fPRpqvrjo0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.optimize as sciopt\n",
        "import scipy.stats as scistats\n",
        "from scipy.stats import multivariate_normal, norm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyzyiYWff-K-"
      },
      "source": [
        "# Вспоминаем и совершенствуем методологию ММП"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb8FN7NvwJEa"
      },
      "source": [
        "## Модель независимых наблюдений"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL_0XQlcwsAX"
      },
      "source": [
        "### Модель\n",
        "\n",
        "Для примера рассмотрим гауссовскую выборку."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZmLZKgqwuw7"
      },
      "source": [
        "$X_i \\overset{iid}{\\sim} \\mathcal{N}(\\mu, \\sigma^2)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiaVXUtfwvdz"
      },
      "source": [
        "### Параметры"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN2th277wxt_"
      },
      "source": [
        "$\\theta = (\\mu, \\sigma^2)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiTQ-1e9xANn"
      },
      "source": [
        "### Выборка"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGc_G14xxBzx"
      },
      "source": [
        "$\\{x_i\\}_{i=1}^{n}$, где $x_i$ - реализация случайной величины $X_i$.  \n",
        "\n",
        "Не забываем, что данные случайные, оценка параметров тоже случайная, но если выборка дана, оценка может быть вычислена по ней."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owouk4gGwz3f"
      },
      "source": [
        "### Правдоподобие\n",
        "\n",
        "Независимость наблюдений позволяет разложить совместную плотность в произведение плотностей, после логарифмирования получим лог-правдоподобие в виде суммы логарифмов:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV1xLcuIw22O"
      },
      "source": [
        "$L(\\theta) = p_{\\theta}(x_1, ..., x_n) = \\prod \\limits_{i=1}^{n} p_{\\mathcal{N}( \\mu, \\sigma^2)}(x_i)  = \\prod \\limits_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^2 }} \\exp \\bigl[ - \\frac{(x_i - \\mu)^2}{2 \\sigma^2}\\bigr]$\n",
        "\n",
        "$l(\\theta) = \\sum \\limits_{i=1}^{n} \\ln p_{\\mathcal{N}( \\mu, \\sigma^2)}(x_i)\n",
        "= - \\frac{n}{2} \\ln (2 \\pi \\sigma^2) - \\sum \\limits_{i=1}^n \\frac{(x_i - \\mu)^2}{2 \\sigma^2}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVTqlZlpw3AV"
      },
      "source": [
        "### ММП-оценки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVfGi3sEw4DF"
      },
      "source": [
        "$l'_{\\mu} = - \\sum \\limits_{i=1}^n \\frac{- 2 (x_i - \\mu)}{2 \\sigma^2} = 0$\n",
        "\n",
        "Из этого условия получаем $\\mu = \\frac{1}{n} \\sum \\limits_{i=1}^n x_i$.\n",
        "\n",
        "\n",
        "$l'_{\\sigma^2} = - \\frac{n}{2} \\frac{1}{\\sigma^2}  + \\frac{1}{2 \\sigma^4} \\sum \\limits_{i=1}^n (x_i - \\mu)^2 = 0$\n",
        "\n",
        "\n",
        "Отсюда $\\sigma^2 = \\frac{1}{n} \\sum \\limits_{i=1}^n (x_i - \\mu)^2$.\n",
        "\n",
        "Поскольку формально это не истинные параметры, а их оценки (они зависят от выборки), то обычно вместо $\\mu, \\sigma^2$ пишут $\\hat{\\mu}, \\hat{\\sigma}^2$, про это нельзя забывать.\n",
        "\n",
        "Можно проверить условие второго порядка, вы справитесь, мы в вас верим."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0HdovBoSsTs"
      },
      "source": [
        "### Оценка одномерной выборки\n",
        "\n",
        "Проверим формулы на примере."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8glggT9aSsTt"
      },
      "outputs": [],
      "source": [
        "size = 10000 # Number of observations\n",
        "mu = 3 # Mean\n",
        "sigma = 2 # Standard deviation\n",
        "sample = # ༼ つ ◕_◕ ༽つ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSjyipKCSsTv"
      },
      "source": [
        "### Явное решение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vijZioTQSsTw"
      },
      "outputs": [],
      "source": [
        "mu_hat = # ༼ つ ◕_◕ ༽つ\n",
        "sigma_hat = # ༼ つ ◕_◕ ༽つ\n",
        "\n",
        "print(f\"Estimated mu: {mu_hat}\")\n",
        "print(f\"Estimated sigma: {sigma_hat}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiDYXdklSsTx"
      },
      "source": [
        "### Численное решение\n",
        "\n",
        "В более сложных случаях можно пробовать численно, ниже показано как. В нашем очень простом случае мы имеем возможность сравнить резульаты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9v8hliCdSsTy"
      },
      "outputs": [],
      "source": [
        "def log_likelihood_factory(sample: np.ndarray):\n",
        "\n",
        "    def log_likelihood(params):\n",
        "        mu, sigma = params\n",
        "\n",
        "        lik =  # ༼ つ ◕_◕ ༽つ\n",
        "\n",
        "        # For maximization\n",
        "        return  - lik\n",
        "\n",
        "    return log_likelihood\n",
        "\n",
        "\n",
        "def log_likelihood_grad_factory(sample: np.ndarray):\n",
        "\n",
        "    def log_likelihood_grad(params):\n",
        "        mu, sigma = params\n",
        "\n",
        "        mu_part =  # ༼ つ ◕_◕ ༽つ\n",
        "        sigma_part = # ༼ つ ◕_◕ ༽つ\n",
        "\n",
        "        # For maximization\n",
        "        return - np.array([mu_part, sigma_part])\n",
        "\n",
        "    return log_likelihood_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHCYCuVDSsTz"
      },
      "outputs": [],
      "source": [
        "x0 = np.random.uniform(size=2)\n",
        "res = sciopt.minimize(\n",
        "    log_likelihood_factory(sample),\n",
        "    x0,\n",
        "    jac=log_likelihood_grad_factory(sample)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLDYirqaSsT0"
      },
      "outputs": [],
      "source": [
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O2Gt5rRwJEb"
      },
      "source": [
        "## Авторегрессия\n",
        "\n",
        "Авторегрессия порядка $p$ также обозначается $AR(p)$, известная и хорошо исследованная модель временных рядов, для нас это первый несложный пример выборки из зависимых наблюдений, а ещё -- база для понимания механизма тех моделей, что мы увидим далее."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eX8ZaopxjYa"
      },
      "source": [
        "### Модель"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOqAKwADxnde"
      },
      "source": [
        "$X_0 = x_0 = const$\n",
        "\n",
        "$X_{i+1} = \\alpha \\cdot X_i + \\epsilon_{i+1} $\n",
        "\n",
        "$\\epsilon_i \\overset{iid}{\\sim}  \\mathcal{N}(0, \\sigma^2)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znfdxYd3xnq6"
      },
      "source": [
        "### Параметры"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmsaGJ8dxvPX"
      },
      "source": [
        "$\\theta = (\\alpha, \\sigma^2)$ и $x_0$ обычно полагается равным первому известному наблюдению, либо задаётся априорным распределением (в байесовском подходе). Мы примем первый вариант, оказывается он тоже может быть обоснован с точки зрения гауссовского априорного распределения для $X_0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrnYI5Fkxvm2"
      },
      "source": [
        "### Выборка"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfOwPsThxzYJ"
      },
      "source": [
        "$\\{x_i\\}_{i=0}^{n}$, где $x_i$ - реализация случайной величины $X_i$\n",
        "\n",
        "$\\hat{\\epsilon}_i =x_{i} - \\alpha x_{i-1}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfrfRQq1xzjG"
      },
      "source": [
        "### Правдоподобие"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrOQG-KOJItP"
      },
      "source": [
        "$X_{i+1} = \\alpha \\cdot X_i + \\epsilon_{i+1}, \\quad X_{i+1} | X_i \\sim \\mathcal{N}(\\alpha X_i, \\sigma^2)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chQVQOkXx3a-"
      },
      "source": [
        "<details>\n",
        "\n",
        "<summary>Спойлер</summary>\n",
        "$L(\\theta) = p_{\\theta}(x_1, ..., x_n) = p_{\\theta}(x_1) p_{\\theta}(x_2 | x_1) ... p_{\\theta}(x_n | x_{n-1}, ... x_1)  = \\prod \\limits_{i=1}^n p_{\\theta}(x_i | x_{i-1}) = \\prod \\limits_{i=1}^n p_{\\mathcal{N}(\\alpha x_{i-1}, \\sigma^2)}(x_i)$\n",
        "\n",
        "$L(\\theta) = p_{\\theta}(\\hat{\\epsilon}_1, ..., \\hat{\\epsilon}_n) = \\prod \\limits_{i=1}^n p_{\\theta}(\\hat{\\epsilon}_i)  = \\prod \\limits_{i=1}^n p_{\\mathcal{N}(0, \\sigma^2)}(x_i - \\alpha x_{i-1})$\n",
        "\n",
        "$l(\\theta) = - \\frac{n}{2} \\ln (2 \\pi \\sigma^2) - \\sum \\limits_{i=1}^n \\frac{(x_i - \\alpha \\cdot x_{i-1})^2}{2 \\sigma^2} =  - \\frac{n}{2} \\ln (2 \\pi \\sigma^2) - \\sum \\limits_{i=1}^n \\frac{x_i ^2  - 2\\alpha \\cdot x_i x_{i-1} + (\\alpha \\cdot x_{i-1})^2}{2 \\sigma^2}$\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHsSHC5IsufJ"
      },
      "source": [
        "Как вы думаете, почему в условной вероятности можно убрать все наблюдения в условии кроме самого нового?\n",
        "\n",
        "<details>\n",
        "\n",
        "<summary>Спойлер</summary>\n",
        "Это можно проверить, выписав $X_i$ в виде суммы шумов и сопоставив с другими $X_j$.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCdouHY6x3po"
      },
      "source": [
        "### ММП-оценки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czz0onbXx4ol"
      },
      "source": [
        "<details>\n",
        "\n",
        "<summary>Спойлер</summary>\n",
        "$l'_{\\alpha} =  - \\sum \\limits_{i=1}^n \\frac{ - 2 \\cdot x_i x_{i-1} + 2\\alpha \\cdot x_{i-1}^2}{2 \\sigma^2}\n",
        "= -\\frac{1}{ \\sigma^2} [ - \\sum \\limits_{i=1}^n x_i x_{i-1} + \\alpha \\cdot \\sum \\limits_{i=1}^n x_{i-1}^2 ]= 0$\n",
        "\n",
        "$\\hat{\\alpha}_{MLE} = \\frac{\\sum \\limits_{i=1}^n x_i x_{i-1} }{ \\sum \\limits_{i=1}^n x_{i-1}^2}$\n",
        "\n",
        "$l'_{\\sigma^2} = - \\frac{n}{2} \\frac{1}{\\sigma^2}  + \\frac{1}{2\\sigma^4} \\sum \\limits_{i=1}^n (x_i - \\alpha \\cdot x_{i-1})^2 = 0$\n",
        "\n",
        "$\\hat{\\sigma}^2_{MLE} = \\frac{1}{n} \\sum \\limits_{i=1}^n (x_i - \\hat{\\alpha} \\cdot x_{i-1})^2$\n",
        "</details>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdrLx7MaB8AJ"
      },
      "outputs": [],
      "source": [
        "class AR_MLE:\n",
        "    \"\"\"\n",
        "    Класс авторегрессионной модели (AR(1)) с аналитической оценкой\n",
        "    параметров методом максимального правдоподобия (MLE).\n",
        "    \"\"\"\n",
        "    def __init__(self, p: int):\n",
        "        \"\"\"\n",
        "        Инициализирует модель с заданным порядком (p).\n",
        "\n",
        "        Args:\n",
        "            p (int): Порядок авторегрессии. Должен быть 1 для этого класса.\n",
        "        \"\"\"\n",
        "        if p != 1:\n",
        "            raise ValueError(\"Этот класс предназначен только для модели AR(1).\")\n",
        "        self.p = p\n",
        "        self.coefficients = None\n",
        "        self.sigma2 = None  # Дисперсия шума\n",
        "        self.fitted_data = None\n",
        "        self.training_history = None # Данные, использованные для обучения\n",
        "        self.full_history = None # Полные исходные данные\n",
        "\n",
        "    def sample(self, coefficients: np.ndarray, length: int, initial_values: np.ndarray = None, std_dev: float = 1.0) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Генерирует выборку данных из модели AR(1) с заданными коэффициентами (без константы).\n",
        "\n",
        "        Args:\n",
        "            coefficients (np.ndarray): Коэффициенты модели.\n",
        "            length (int): Длина сгенерированного временного ряда.\n",
        "            initial_values (np.ndarray, optional): Начальные значения для ряда.\n",
        "                Если не предоставлены, используются случайные значения.\n",
        "            std_dev (float, optional): Стандартное отклонение шума.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Сгенерированный временной ряд.\n",
        "        \"\"\"\n",
        "        if coefficients.size != self.p:\n",
        "             raise ValueError(f\"Неверное количество коэффициентов. Ожидается {self.p}.\")\n",
        "\n",
        "        if initial_values is None:\n",
        "            data = np.random.randn(self.p) * std_dev\n",
        "        else:\n",
        "            if len(initial_values) < self.p:\n",
        "                raise ValueError(f\"Требуется как минимум {self.p} начальных значений.\")\n",
        "            data = initial_values[:self.p].copy()\n",
        "\n",
        "        for _ in range(length - self.p):\n",
        "            lag_values = data[-self.p:]\n",
        "            next_value = np.dot(coefficients, lag_values[::-1]) # + ༼ つ ◕_◕ ༽つ\n",
        "            data = np.append(data, next_value)\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "    def fit(self, data: np.ndarray, n_predict: int = 0):\n",
        "        \"\"\"\n",
        "        Аналитическая оценка параметров модели AR(1).\n",
        "        Обучение производится на первых len(data) - n_predict шагах.\n",
        "\n",
        "        Args:\n",
        "            data (np.ndarray): Данные для обучения.\n",
        "            n_predict (int): Количество шагов, которые будут использоваться для предсказания\n",
        "                             и не будут включены в обучение.\n",
        "        \"\"\"\n",
        "        if len(data) < 2 + n_predict:\n",
        "            raise ValueError(f\"Длина данных ({len(data)}) должна быть больше порядка p + n_predict ({self.p + n_predict}).\")\n",
        "\n",
        "        self.full_history = data.copy()\n",
        "        training_data_length = len(data) - n_predict\n",
        "        self.training_history = data[:training_data_length].copy()\n",
        "\n",
        "        n = len(self.training_history)\n",
        "\n",
        "        self.coefficients = # ༼ つ ◕_◕ ༽つ\n",
        "\n",
        "        residuals = # ༼ つ ◕_◕ ༽つ\n",
        "        self.sigma2 = np.sum(residuals**2) / (n - 1) # Здесь n - это len(training_history)\n",
        "\n",
        "        # Вычисляем подогнанные значения для тренировочного набора\n",
        "        self._calculate_fitted_data(self.training_history)\n",
        "\n",
        "        print(\"Обучение завершено методом MLE для AR(1).\")\n",
        "        print(f\"Обучение проводилось на {training_data_length} точках.\")\n",
        "        print(f\"Оценка alpha: {self.coefficients[0]:.4f}\")\n",
        "        print(f\"Оценка дисперсии шума sigma^2: {self.sigma2:.4f}\")\n",
        "\n",
        "\n",
        "    def _calculate_fitted_data(self, data: np.ndarray):\n",
        "        \"\"\"\n",
        "        Вычисляет подогнанные значения на основе оцененных параметров.\n",
        "        \"\"\"\n",
        "        if self.coefficients is None:\n",
        "             raise ValueError(\"Модель не обучена. Оцените параметры перед вычислением подогнанных значений.\")\n",
        "\n",
        "        n_obs = len(data)\n",
        "        self.fitted_data = np.zeros(n_obs)\n",
        "        # Первое значение подогнанного ряда равно историческому значению\n",
        "        self.fitted_data[0] = data[0]\n",
        "\n",
        "\n",
        "        # Вычисление подогнанных значений для t >= 1\n",
        "        for t in range(1, n_obs):\n",
        "            predicted_value = # ༼ つ ◕_◕ ༽つ\n",
        "            self.fitted_data[t] = predicted_value\n",
        "\n",
        "\n",
        "    def plot(self, confidence_level: float = 0.95, start: int = None, end: int = None, n_predict: int = 0):\n",
        "        \"\"\"\n",
        "        Строит график исходных (полных) данных, подогнанных данных на обучающей выборке,\n",
        "        доверительный интервал для подогнанных значений и предсказания на n_predict шагов вперед\n",
        "        с соответствующим доверительным интервалом, сравнивая их с исходными данными.\n",
        "\n",
        "        Args:\n",
        "            confidence_level (float): Доверительная вероятность (от 0 до 1).\n",
        "            start (int, optional): Начальный индекс отрисовки.\n",
        "            end (int, optional): Конечный индекс отрисовки.\n",
        "            n_predict (int): Количество шагов для предсказания. Должен быть равен n_predict,\n",
        "                             использованному в методе fit.\n",
        "        \"\"\"\n",
        "        if self.fitted_data is None or self.sigma2 is None or self.full_history is None or self.training_history is None:\n",
        "            print(\"Модель не обучена должным образом. Убедитесь, что вы вызвали метод `fit`.\")\n",
        "            return\n",
        "\n",
        "        total_length = len(self.full_history)\n",
        "        training_length = len(self.training_history)\n",
        "        prediction_start_idx_full_history = training_length # Индекс в full_history, откуда начинаются предсказываемые данные\n",
        "\n",
        "        if n_predict != total_length - training_length:\n",
        "             print(f\"Предупреждение: n_predict ({n_predict}) в plot не совпадает с количеством точек вне обучения ({total_length - training_length}).\")\n",
        "\n",
        "\n",
        "        # Определяем полный диапазон отрисовки с учетом предсказаний\n",
        "        plot_end_total = total_length + n_predict # График может идти до total_length + n_predict\n",
        "\n",
        "        # Определяем индексы начала и конца для всего графика\n",
        "        if start is None:\n",
        "            plot_start_idx = 0\n",
        "        else:\n",
        "            plot_start_idx = max(0, start)\n",
        "\n",
        "        if end is None:\n",
        "            plot_end_idx = plot_end_total\n",
        "        else:\n",
        "            plot_end_idx = min(plot_end_total, end)\n",
        "\n",
        "        if plot_start_idx >= plot_end_idx:\n",
        "            print(\"Некорректный срез: начальный индекс больше или равен конечному.\")\n",
        "            return\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # --- Отрисовка исходных данных (полная история) ---\n",
        "        # Отрисовываем весь диапазон исходных данных, который попадает в запрошенный срез plot_start_idx:plot_end_idx\n",
        "        history_plot_start = max(plot_start_idx, 0)\n",
        "        history_plot_end = min(plot_end_idx, total_length)\n",
        "        if history_plot_start < history_plot_end:\n",
        "             plt.plot(range(history_plot_start, history_plot_end), self.full_history[history_plot_start:history_plot_end], label='Исходные данные', color='gray', alpha=0.6)\n",
        "\n",
        "        # --- Предсказания ---\n",
        "        if n_predict > 0:\n",
        "            # Берем последнее значение из обучающей выборки для старта предсказаний\n",
        "            last_observed_value = self.training_history[-1]\n",
        "            predictions = [last_observed_value] # Начинаем с последнего наблюдаемого значения обучающей выборки\n",
        "\n",
        "            for _ in range(n_predict):\n",
        "                next_prediction = # ༼ つ ◕_◕ ༽つ\n",
        "                predictions.append(next_prediction)\n",
        "\n",
        "            # Ограничиваем диапазон отрисовки предсказаний согласно plot_start_idx и plot_end_idx\n",
        "            plot_prediction_start = max(plot_start_idx, training_length - 1)\n",
        "            # Ensure plot_prediction_end does not exceed the actual range of generated predictions\n",
        "            plot_prediction_end = min(plot_end_idx, training_length + n_predict)\n",
        "\n",
        "            if plot_prediction_start < plot_prediction_end:\n",
        "                # Индексы для среза predictions относительно начала предсказаний (training_length - 1)\n",
        "                predictions_slice_start = plot_prediction_start - (training_length - 1)\n",
        "                predictions_slice_end = predictions_slice_start + (plot_prediction_end - plot_prediction_start)\n",
        "\n",
        "                plt.plot(range(plot_prediction_start, plot_prediction_end),\n",
        "                         predictions[predictions_slice_start : predictions_slice_end],\n",
        "                         label=f'Предсказания ({n_predict} шагов)', color='red', linestyle='-')\n",
        "\n",
        "\n",
        "                # Доверительный интервал для предсказаний\n",
        "                # Дисперсия предсказания увеличивается с каждым шагом\n",
        "                forecast_variances = np.zeros(n_predict + 1) # +1 для последней исторической точки (дисперсия 0)\n",
        "                forecast_variances[0] = 0 # Дисперсия на шаге 0 (последнее наблюдение)\n",
        "\n",
        "                for h in range(1, n_predict + 1):\n",
        "                     sum_of_squares_phi = np.sum(self.coefficients[0]**(2 * np.arange(h))) # Sum_{j=0}^{h-1} phi^{2j}\n",
        "                     forecast_variances[h] = self.sigma2 * sum_of_squares_phi\n",
        "\n",
        "                forecast_std_devs = np.sqrt(forecast_variances)\n",
        "                z_score = norm.ppf(1 - (1 - confidence_level) / 2)\n",
        "\n",
        "                # Вычисление границ доверительного интервала для предсказаний\n",
        "                lower_bound_pred = np.array(predictions) - z_score * forecast_std_devs\n",
        "                upper_bound_pred = np.array(predictions) + z_score * forecast_std_devs\n",
        "\n",
        "                # Отрисовка затенённого доверительного интервала для предсказаний\n",
        "                plt.fill_between(range(plot_prediction_start, plot_prediction_end),\n",
        "                                 lower_bound_pred[predictions_slice_start : predictions_slice_end],\n",
        "                                 upper_bound_pred[predictions_slice_start : predictions_slice_end],\n",
        "                                 color='red', alpha=0.1, label=f'{int(confidence_level*100)}% Доверительный интервал предсказаний')\n",
        "                # Отрисовываем исходные данные (часть, которая сравнивается с предсказаниями)\n",
        "                # Этот участок исходных данных находится после training_length - 1\n",
        "                history_prediction_start = max(plot_start_idx, training_length - 1) # Начало участка сравнения в полном ряду\n",
        "                history_prediction_end = min(plot_end_idx, total_length) # Конец участка сравнения в полном ряду\n",
        "\n",
        "\n",
        "                if history_prediction_start < history_prediction_end: # Проверяем, что есть исторические данные в этом диапазоне\n",
        "                     plt.plot(range(history_prediction_start, history_prediction_end),\n",
        "                              self.full_history[history_prediction_start:history_prediction_end],\n",
        "                              label='Исходные данные (предсказание)', color='green', alpha=0.6)\n",
        "\n",
        "\n",
        "        plt.title(f'Авторегрессионная модель AR({self.p})')\n",
        "        plt.xlabel('Время')\n",
        "        plt.ylabel('Значение')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLTXB7etuhS8"
      },
      "source": [
        "Запустим!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTQS9zqjugeZ"
      },
      "outputs": [],
      "source": [
        "### Пример использования класса\n",
        "\n",
        "# 1. Создание и выборка данных\n",
        "ar_p = 1\n",
        "true_coefficients = np.array([0.8])\n",
        "true_std_dev = 0.4\n",
        "\n",
        "ar_model = AR_MLE(p=ar_p)\n",
        "# Генерируем данные\n",
        "sample_data = ar_model.sample(coefficients=true_coefficients, length=200, std_dev=true_std_dev, initial_values=np.array([0.0])) # Начальное значение X_0 = 0\n",
        "\n",
        "# 2. Обучение модели с использованием явных формул для AR(1)\n",
        "# Указываем n_predict, чтобы исключить последние точки из обучения\n",
        "n_predict_example = 20\n",
        "ar_model.fit(data=sample_data, n_predict=n_predict_example)\n",
        "\n",
        "# 3. Построение графика с доверительным интервалом\n",
        "ar_model.plot(confidence_level=0.95, start=150, end=220, n_predict=n_predict_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WatTSapVXT1S"
      },
      "source": [
        "## Многомерная гауссовская выборка"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8ddGR3Uv1c-"
      },
      "source": [
        "Что если наблюдения -- векторы, но всё ещё независимые между друг другом? Для иллюстраций рассмотрим 2d, а формулы напишем в общем виде.\n",
        "\n",
        "### Модель\n",
        "\n",
        "### Правдоподобие\n",
        "\n",
        "$X_{i} \\sim^{iid} \\mathcal{N}(\\mu, \\Sigma)$\n",
        "\n",
        "Теперь в параметрах у нас вектор матожидания $\\mu$ и ковариационная матрица $\\Sigma$, но правдоподобие в силу независимости пишется точно так же.\n",
        "\n",
        "$$\n",
        "L(\\theta) = p_{\\theta}(x_1, ..., x_n) =  \\prod \\limits_{i=1}^n p_{\\mathcal{N}(\\mu, \\Sigma)}(x_i)\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "l(\\theta) = const - \\frac{n}{2} \\ln (\\det \\Sigma) - \\frac{1}{2}\\sum_{i=1}^n (x_i - \\mu)^T \\Sigma^{-1} (x_i - \\mu).\n",
        "$$\n",
        "\n",
        "Для оценки параметров нам понадобится продифференцировать лог-правдоподобие по матрице и приравнять производные к нулю.\n",
        "\n",
        "\n",
        "\n",
        "Квадратичные формы очень удобно представлять через след, который мы уже умеем дифференцировать:\n",
        "\n",
        "$$\n",
        "x^T Q x = \\text{Tr} (Qx x^T).\n",
        "$$\n",
        "\n",
        "Считаем производную по $\\mu$ (на доске), приравниваем к нулю и...\n",
        "\n",
        "<details>\n",
        "\n",
        "<summary>Спойлер</summary>\n",
        "$$\n",
        "\\mu = \\frac{1}{n}\\sum_{i=1} x_i = \\overline{x},\n",
        "$$\n",
        "\n",
        "что достаточно логично. В случае $\\Sigma$ удобно воспользоваться свойством детерминанта\n",
        "\n",
        "$$\n",
        "det(A^{-1}) = 1/det(A)\n",
        "$$\n",
        "\n",
        "и дифференцировать по $\\Sigma^{-1}$. Немного расчётов (см. доску) и...\n",
        "\n",
        "$$\n",
        "\\Sigma = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\mu)(X_i - \\mu)^T,\n",
        "$$\n",
        "\n",
        "где $\\mu$ мы уже нашли ранее.\n",
        "</details>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOl5ggIsXV89"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MultivariateNormalMLE:\n",
        "    \"\"\"\n",
        "    Класс для многомерного нормального распределения с оценкой параметров MLE.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.mean = None\n",
        "        self.covariance = None\n",
        "\n",
        "    def sample(self, mean: np.ndarray, covariance: np.ndarray, n_samples: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Генерирует выборку из многомерного нормального распределения.\n",
        "\n",
        "        Args:\n",
        "            mean (np.ndarray): Вектор среднего.\n",
        "            covariance (np.ndarray): Ковариационная матрица.\n",
        "            n_samples (int): Количество генерируемых выборок.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Сгенерированные выборки (n_samples, n_features).\n",
        "        \"\"\"\n",
        "        return # ༼ つ ◕_◕ ༽つ\n",
        "\n",
        "    def fit(self, data: np.ndarray):\n",
        "        \"\"\"\n",
        "        Оценивает параметры (среднее и ковариацию) методом MLE.\n",
        "\n",
        "        Args:\n",
        "            data (np.ndarray): Данные для обучения (n_samples, n_features).\n",
        "        \"\"\"\n",
        "        # Проверка размера данных\n",
        "        if data.ndim < 2:\n",
        "            raise ValueError(\"Данные должны быть двумерным массивом (n_samples, n_features).\")\n",
        "\n",
        "        n_samples, n_features = data.shape\n",
        "\n",
        "        # Оценка среднего\n",
        "        self.mean = # ༼ つ ◕_◕ ༽つ\n",
        "\n",
        "        # Оценка ковариации\n",
        "\n",
        "        self.covariance =  # ༼ つ ◕_◕ ༽つ\n",
        "\n",
        "        print(\"Оценка MLE завершена.\")\n",
        "        print(f\"Оцененное среднее:\\n{self.mean}\")\n",
        "        print(f\"Оцененная ковариация:\\n{self.covariance}\")\n",
        "\n",
        "    def plot(self, data: np.ndarray, confidence_level: float = 0.95):\n",
        "        \"\"\"\n",
        "        Визуализирует данные и доверительный эллипс (только для 2D).\n",
        "\n",
        "        Args:\n",
        "            data (np.ndarray): Исходные данные.\n",
        "            confidence_level (float): Уровень доверия для эллипса.\n",
        "        \"\"\"\n",
        "        if self.mean is None or self.covariance is None:\n",
        "            raise ValueError(\"Модель не обучена. Запустите метод `fit`.\")\n",
        "\n",
        "        if data.shape[1] != 2:\n",
        "            print(\"Визуализация поддерживается только для 2D данных.\")\n",
        "            return\n",
        "\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.scatter(data[:, 0], data[:, 1], alpha=0.5, label='Исходные данные')\n",
        "\n",
        "        # Построение доверительного эллипса\n",
        "        eigenvalues, eigenvectors = np.linalg.eigh(self.covariance)\n",
        "        order = eigenvalues.argsort()[::-1]\n",
        "        eigenvalues = eigenvalues[order]\n",
        "        eigenvectors = eigenvectors[:, order]\n",
        "\n",
        "        # Угол поворота эллипса\n",
        "        angle = np.degrees(np.arctan2(*eigenvectors[:, 0][::-1]))\n",
        "\n",
        "        # Квантиль для chi-squared распределения\n",
        "        # df=2 для двумерного распределения\n",
        "        chi_squared_val = scistats.chi2.ppf(confidence_level, df=2)\n",
        "\n",
        "        # Длина полуосей\n",
        "        width, height = 2 * np.sqrt(chi_squared_val) * np.sqrt(eigenvalues)\n",
        "\n",
        "        from matplotlib.patches import Ellipse\n",
        "        ellipse = Ellipse(xy=self.mean, width=width, height=height, angle=angle,\n",
        "                          facecolor='none', edgecolor='red', lw=2, label=f'{int(confidence_level*100)}% Доверительный эллипс')\n",
        "\n",
        "        ax = plt.gca()\n",
        "        ax.add_patch(ellipse)\n",
        "\n",
        "        # Центр распределения\n",
        "        plt.plot(self.mean[0], self.mean[1], 'ro', label='Оцененное среднее')\n",
        "\n",
        "        plt.title('Многомерное нормальное распределение')\n",
        "        plt.xlabel('Переменная 1')\n",
        "        plt.ylabel('Переменная 2')\n",
        "        plt.legend()\n",
        "        plt.axis('equal')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CQhHLKc8ENS"
      },
      "outputs": [],
      "source": [
        "### Пример использования класса\n",
        "\n",
        "# 1. Создание экземпляра класса\n",
        "mle_model = MultivariateNormalMLE()\n",
        "\n",
        "# 2. Определение истинных параметров и генерация данных\n",
        "true_mean = np.array([2, 3])\n",
        "true_covariance = np.array([[1.0, 0.8],\n",
        "                             [0.8, 1.5]])\n",
        "n_samples = 500\n",
        "\n",
        "sample_data = mle_model.sample(true_mean, true_covariance, n_samples)\n",
        "\n",
        "# 3. Оценка параметров на основе сгенерированных данных\n",
        "mle_model.fit(sample_data)\n",
        "\n",
        "# 4. Визуализация данных и доверительного эллипса\n",
        "mle_model.plot(sample_data, confidence_level=0.95)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N1Z9Qd2T23Z"
      },
      "source": [
        "## Векторная авторегрессия\n",
        "\n",
        "Модель векторной авторегрессии, обозначаемая $VAR(p)$ -- это та же $AR(p)$, но теперь $X$ и $\\varepsilon$ -- это векторы, а коэффициент перед лагом -- квадратная матрица $A$. До этого мы научились работать с зависимостями в правдоподобии, научились строить оценки для векторного гауссовского случая, теперь -- попробуем комбо.\n",
        "\n",
        "### Модель\n",
        "\n",
        "$X_0 = x_0 = const \\in \\mathbb{R}^d$\n",
        "\n",
        "$X_{i+1} = A X_i + \\epsilon_{i+1} $\n",
        "\n",
        "$\\epsilon_i \\overset{iid}{\\sim}  \\mathcal{N}(0, \\Sigma)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Wtf_BRs4VMe"
      },
      "source": [
        "### Правдоподобие\n",
        "\n",
        "$X_{i+1} = A X_i + \\epsilon_{i+1}, \\quad X_{i+1} | X_i \\sim \\mathcal{N}(A X_i, \\Sigma)$\n",
        "\n",
        "В общем, всё то же, что мы уже видели, но теперь параметры сложнее:\n",
        "<details>\n",
        "\n",
        "<summary>Спойлер</summary>\n",
        "$$\n",
        "L(\\theta) = p_{\\theta}(x_1, ..., x_n) = p_{\\theta}(x_1) p_{\\theta}(x_2 | x_1) ... p_{\\theta}(x_n | x_{n-1}, ... x_1)  = \\prod \\limits_{i=0}^{n-1} p_{\\theta}(x_{i+1} | x_{i}) = \\prod \\limits_{i=0}^{n-1} p_{\\mathcal{N}(A x_{i-1}, \\Sigma)}(x_{i+1})\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "l(\\theta) = const - \\frac{n}{2} \\ln (\\det \\Sigma) - \\frac{1}{2}\\sum_{i=0}^{n-1} (x_{i+1} - A x_{i})^T \\Sigma^{-1} (x_{i+1} - A x_{i})\n",
        "$$\n",
        "</details>\n",
        "\n",
        "\n",
        "Сразу видим, что приём с дифференцированием по $\\Sigma^{-1}$ даёт нам уже известную формулу, н ов более сложной постановке:\n",
        "\n",
        "<details>\n",
        "\n",
        "<summary>Спойлер</summary>\n",
        "$$\n",
        "\\Sigma = \\frac{1}{n-1} \\sum_{i=0}^{n-1} (x_{i+1} - Ax_i)(x_{i+1} - Ax_i)^T.\n",
        "$$\n",
        "\n",
        "Для дифференцирования по $A$ придётся расписать квадратичную форму чуть подробнее, так как там будут кросс-слагаемые.\n",
        "\n",
        "$$\n",
        "\\frac{1}{2}\\sum_{i=0}^{n-1} (x_{i+1} - A x_{i})^T \\Sigma^{-1} (x_{i+1} - A x_{i}) =\\frac{1}{2}\\sum_{i=0}^{n-1} \\text{Tr}\\left( \\Sigma^{-1}\\left(x_{i+1}x_{i+1}^T + A x_{i} x_{i}^T A^T - A x_{i} x_{i+1}^T - x_{i+1} x_i^T A^T \\right)\\right)\n",
        "$$\n",
        "\n",
        "Под знаком суммы первое слагаемое не зависит от $A$. Второе слагаемое даёт после дифференцирования\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\text{Tr}(\\Sigma^{-1} Ax_ix_i^TA^T)}{\\partial A^T} = ...\\text{в этот момент загляните в лекцию, если не понимаете, как}... = 2x_ix_i^TA^T\\Sigma^{-1},\n",
        "$$\n",
        "\n",
        "а третье и четвёртое -- это привычный уже след.\n",
        "\n",
        "Собрав вместе и сократив справа на $\\Sigma^{-1}$ получим из условия первого порядка\n",
        "\n",
        "$$\n",
        "A = \\sum_{i=0}^{n-1} x_{i+1}x_{i}^T (\\sum_{i=0}^{n-1} x_ix_i^T)^{-1}.\n",
        "$$\n",
        "</details>\n",
        "\n",
        "\n",
        "Если говорить о вычислениях, можно поступить сильно проще. Вот как:\n",
        "<details>\n",
        "<summary>Спойлер</summary>\n",
        "Может показаться странным, что мы выводим все эти формулы ведь это просто задача наименьших квадратов, которую нас учат решать через ```np.linalg.lstsq```. В поставленной нами задаче действительно так, но мы увидим, что это не всегда так удобно, как здесь.\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZN_dAGD36RU"
      },
      "outputs": [],
      "source": [
        "class VAR1_MLE:\n",
        "    \"\"\"\n",
        "    Класс векторной авторегрессии (VAR(1)) с оценкой параметров\n",
        "    методом максимального правдоподобия (MLE).\n",
        "    \"\"\"\n",
        "    def __init__(self, K: int):\n",
        "        \"\"\"\n",
        "        Инициализирует модель VAR(1) с заданным количеством переменных.\n",
        "\n",
        "        Args:\n",
        "            K (int): Количество временных рядов.\n",
        "        \"\"\"\n",
        "        if K < 1:\n",
        "            raise ValueError(\"Количество переменных K должно быть как минимум 1.\")\n",
        "        self.p = 1 # VAR(1)\n",
        "        self.K = K  # Размерность временного ряда\n",
        "        self.A = None # Матрица коэффициентов - будет оцениваться\n",
        "        self.cov_matrix = None\n",
        "        self.full_history = None # Полные исходные данные\n",
        "        self.training_history = None # Данные, использованные для обучения\n",
        "        self.fitted_data = None\n",
        "\n",
        "    def sample(self, A: np.ndarray, length: int = 100, initial_values: np.ndarray = None, Sigma: np.ndarray = None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Генерирует выборку данных из модели VAR(1) с использованием заданной матрицы коэффициентов.\n",
        "\n",
        "        Args:\n",
        "            A (np.ndarray): Матрица коэффициентов (K, K).\n",
        "            length (int): Длина сгенерированного временного ряда.\n",
        "            initial_values (np.ndarray, optional): Начальные значения для ряда (K,).\n",
        "                Если не предоставлены, используются случайные значения.\n",
        "            Sigma (np.ndarray, optional): Ковариационная матрица шума.\n",
        "                Если не предоставлена, используется единичная матрица.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Сгенерированный многомерный временной ряд (length, K).\n",
        "        \"\"\"\n",
        "        if A.shape != (self.K, self.K):\n",
        "            raise ValueError(f\"Неверная форма матрицы коэффициентов A. Ожидается ({self.K}, {self.K}).\")\n",
        "        if Sigma is None:\n",
        "            Sigma = np.eye(self.K)\n",
        "        if Sigma.shape != (self.K, self.K):\n",
        "            raise ValueError(f\"Неверная форма ковариационной матрицы. Ожидается ({self.K}, {self.K}).\")\n",
        "\n",
        "\n",
        "        data = np.zeros((length, self.K))\n",
        "\n",
        "        if initial_values is None:\n",
        "            data[0] = np.random.multivariate_normal(np.zeros(self.K), Sigma)\n",
        "        else:\n",
        "            if len(initial_values) != self.K:\n",
        "                 raise ValueError(f\"Неверное количество начальных значений. Ожидается {self.K}.\")\n",
        "            data[0] = initial_values.copy()\n",
        "\n",
        "\n",
        "        # Генерация остальных наблюдений\n",
        "        for t in range(1, length):\n",
        "            generated = # ༼ つ ◕_◕ ༽つ\n",
        "            noise = # ༼ つ ◕_◕ ༽つ\n",
        "            data[t] = generated + noise\n",
        "\n",
        "        return data\n",
        "\n",
        "    def fit(self, data: np.ndarray, n_predict: int = 0):\n",
        "        \"\"\"\n",
        "        Оценивает матрицу коэффициентов A и ковариационную матрицу\n",
        "        шума VAR(1) на основе предоставленных данных методом MLE.\n",
        "        Обучение производится на первых len(data) - n_predict шагах.\n",
        "\n",
        "        Args:\n",
        "            data (np.ndarray): Временной ряд (length, K).\n",
        "            n_predict (int): Количество шагов, которые будут использоваться для предсказания\n",
        "                             и не будут включены в обучение.\n",
        "        \"\"\"\n",
        "        if len(data) < 2 + n_predict:\n",
        "            raise ValueError(f\"Длина данных ({len(data)}) должна быть больше 1 + n_predict ({1 + n_predict}).\")\n",
        "        if data.shape[1] != self.K:\n",
        "            raise ValueError(f\"Размерность данных ({data.shape[1]}) не соответствует количеству переменных ({self.K}).\")\n",
        "\n",
        "        self.full_history = data.copy()\n",
        "        training_data_length = len(data) - n_predict\n",
        "        self.training_history = data[:training_data_length].copy()\n",
        "\n",
        "        T = len(self.training_history)\n",
        "\n",
        "        x_t = self.training_history[1:, :].T       # x_1, x_2, ..., x_{T-1}\n",
        "        x_prev = self.training_history[:-1, :].T   # x_0, x_1, ..., x_{T-2}\n",
        "\n",
        "        # Оценка матрицы A\n",
        "        try:\n",
        "            self.A = # ༼ つ ◕_◕ ༽つ\n",
        "        except np.linalg.LinAlgError:\n",
        "            raise ValueError(\"Невозможно оценить матрицу A: сумма квадратов предыдущих значений вырождена.\")\n",
        "\n",
        "\n",
        "        # Оценка ковариационной матрицы шума Sigma\n",
        "\n",
        "        residuals = # ༼ つ ◕_◕ ༽つ\n",
        "\n",
        "        sum_res_outer_prod = residuals @ residuals.T # Сумма outer произведений остатков\n",
        "        num_observations = T - 1 # Количество наблюдений для расчета остатков\n",
        "        self.cov_matrix = sum_res_outer_prod / num_observations\n",
        "\n",
        "        print(\"Обучение VAR(1) завершено методом MLE (явные формулы, без константы).\")\n",
        "        print(f\"Обучение проводилось на {training_data_length} точках.\")\n",
        "        print(f\"Оцененная матрица коэффициентов A:\\n{self.A}\")\n",
        "        print(f\"Оцененная ковариационная матрица шума:\\n{self.cov_matrix}\")\n",
        "\n",
        "        # Сохраняем подогнанные значения (для training_history)\n",
        "        self._calculate_fitted_data(self.training_history)\n",
        "\n",
        "\n",
        "    def _calculate_fitted_data(self, training_data: np.ndarray):\n",
        "        \"\"\"Вычисляет подогнанные значения на основе оцененных параметров для тренировочных данных.\"\"\"\n",
        "        if self.A is None:\n",
        "             raise ValueError(\"Модель не обучена. Оцените параметры перед вычислением подогнанных значений.\")\n",
        "\n",
        "        n_obs = len(training_data)\n",
        "        self.fitted_data = np.zeros((n_obs, self.K))\n",
        "        # Первое значение подогнанного ряда равно историческому значению.\n",
        "        self.fitted_data[0] = training_data[0]\n",
        "\n",
        "\n",
        "        # Вычисление подогнанных значений для t >= 1\n",
        "        for t in range(1, n_obs):\n",
        "            predicted_value = # ༼ つ ◕_◕ ༽つ\n",
        "            self.fitted_data[t] = predicted_value\n",
        "\n",
        "\n",
        "    def plot(self, confidence_level: float = 0.95, start: int = None, end: int = None, n_predict: int = 0):\n",
        "        \"\"\"\n",
        "        Строит графики для каждого временного ряда с доверительными интервалами,\n",
        "        включая предсказания.\n",
        "\n",
        "        Args:\n",
        "            confidence_level (float): Доверительная вероятность (от 0 до 1).\n",
        "            start (int, optional): Начальный индекс для отрисовки.\n",
        "            end (int, optional): Конечный индекс для отрисовки.\n",
        "            n_predict (int): Количество шагов для предсказания. Должен быть равен n_predict,\n",
        "                             использованному в методе fit.\n",
        "        \"\"\"\n",
        "        if self.fitted_data is None or self.cov_matrix is None or \\\n",
        "           self.full_history is None or self.training_history is None:\n",
        "            print(\"Модель не обучена должным образом. Убедитесь, что вы вызвали метод `fit`.\")\n",
        "            return\n",
        "\n",
        "        total_length = len(self.full_history)\n",
        "        training_length = len(self.training_history)\n",
        "\n",
        "        if n_predict != total_length - training_length:\n",
        "             print(f\"Предупреждение: n_predict ({n_predict}) в plot не совпадает с количеством точек вне обучения ({total_length - training_length}).\")\n",
        "\n",
        "        plot_end_total = total_length + n_predict\n",
        "\n",
        "        if start is None:\n",
        "            plot_start_idx = 0\n",
        "        else:\n",
        "            plot_start_idx = max(0, start)\n",
        "\n",
        "        if end is None:\n",
        "            plot_end_idx = plot_end_total\n",
        "        else:\n",
        "            plot_end_idx = min(plot_end_total, end)\n",
        "\n",
        "        if plot_start_idx >= plot_end_idx:\n",
        "            print(\"Некорректный срез: начальный индекс больше или равен конечному.\")\n",
        "            return\n",
        "\n",
        "        z_score = norm.ppf(1 - (1 - confidence_level) / 2)\n",
        "\n",
        "        fig, axs = plt.subplots(self.K, 1, figsize=(12, 6 * self.K), sharex=True)\n",
        "        if self.K == 1:\n",
        "            axs = [axs]\n",
        "\n",
        "        # --- Отрисовка исходных данных (полная история) ---\n",
        "        history_plot_start = max(plot_start_idx, 0)\n",
        "        history_plot_end = min(plot_end_idx, total_length)\n",
        "        if history_plot_start < history_plot_end:\n",
        "            for i in range(self.K):\n",
        "                axs[i].plot(range(history_plot_start, history_plot_end),\n",
        "                            self.full_history[history_plot_start:history_plot_end, i],\n",
        "                            label=f'Исходные данные (Переменная {i+1})', color='gray', alpha=0.6)\n",
        "\n",
        "        # --- Предсказания ---\n",
        "        if n_predict > 0:\n",
        "            last_observed_values = self.training_history[-1] # K-мерный вектор\n",
        "            predictions = [last_observed_values] # Список K-мерных векторов (начинается с последнего наблюдаемого)\n",
        "\n",
        "            # Расчет ковариаций ошибки предсказания для каждого шага h:\n",
        "            # Var(y_{t+h}|y_t) = Sigma_h = Sigma + A Sigma A^T + A^2 Sigma (A^2)^T + ... + A^{h-1} Sigma (A^{h-1})^T\n",
        "            # Sigma_h = sum_{j=0}^{h-1} (A^j) Sigma (A^j)^T\n",
        "            forecast_covariances = np.zeros((n_predict + 1, self.K, self.K))\n",
        "            forecast_covariances[0] = np.zeros((self.K, self.K)) # Дисперсия на шаге 0 (последнее наблюдение) равна 0\n",
        "\n",
        "            A_power_j = np.eye(self.K) # A^0\n",
        "            for h in range(1, n_predict + 1):\n",
        "                # Предсказываем следующий шаг\n",
        "                next_prediction = # ༼ つ ◕_◕ ༽つ\n",
        "                predictions.append(next_prediction)\n",
        "\n",
        "                # Обновляем ковариацию ошибки предсказания\n",
        "                forecast_covariances[h] = forecast_covariances[h-1] + A_power_j @ self.cov_matrix @ A_power_j.T\n",
        "                A_power_j = self.A @ A_power_j # A^j -> A^(j+1)\n",
        "\n",
        "            predictions_array = np.array(predictions) # (n_predict + 1, K)\n",
        "\n",
        "            # Определяем диапазон отрисовки предсказаний\n",
        "            # Предсказания начинаются с (training_length - 1) в полном ряду (включая последнее наблюдаемое)\n",
        "            plot_prediction_start = max(plot_start_idx, training_length - 1)\n",
        "            plot_prediction_end = min(plot_end_idx, training_length + n_predict)\n",
        "\n",
        "            if plot_prediction_start < plot_prediction_end:\n",
        "                # Индексы для среза predictions_array относительно начала предсказаний (training_length - 1)\n",
        "                predictions_slice_start_idx = plot_prediction_start - (training_length - 1)\n",
        "                predictions_slice_end_idx = predictions_slice_start_idx + (plot_prediction_end - plot_prediction_start)\n",
        "\n",
        "                for i in range(self.K):\n",
        "                    axs[i].plot(range(plot_prediction_start, plot_prediction_end),\n",
        "                                predictions_array[predictions_slice_start_idx : predictions_slice_end_idx, i],\n",
        "                                label=f'Предсказания ({n_predict} шагов) (Переменная {i+1})', color='red', linestyle='-')\n",
        "\n",
        "                    # Вычисляем доверительный интервал для предсказаний\n",
        "                    # Стандартные отклонения берутся из диагональных элементов ковариационной матрицы предсказания\n",
        "                    forecast_std_devs = np.sqrt(np.array([np.diag(fcov)[i] for fcov in forecast_covariances[predictions_slice_start_idx : predictions_slice_end_idx]]))\n",
        "                    lower_bound_pred = predictions_array[predictions_slice_start_idx : predictions_slice_end_idx, i] - z_score * forecast_std_devs\n",
        "                    upper_bound_pred = predictions_array[predictions_slice_start_idx : predictions_slice_end_idx, i] + z_score * forecast_std_devs\n",
        "\n",
        "                    axs[i].fill_between(range(plot_prediction_start, plot_prediction_end),\n",
        "                                         lower_bound_pred, upper_bound_pred,\n",
        "                                         color='red', alpha=0.1, label=f'{int(confidence_level*100)}% Доверительный интервал предсказаний')\n",
        "\n",
        "                # Отрисовываем исходные данные (часть, которая сравнивается с предсказаниями)\n",
        "                # Этот участок исходных данных находится после training_length - 1\n",
        "                history_prediction_start = max(plot_start_idx, training_length - 1) # Начало участка сравнения в полном ряду\n",
        "                history_prediction_end = min(plot_end_idx, total_length) # Конец участка сравнения в полном ряду\n",
        "\n",
        "\n",
        "                if history_prediction_start < history_prediction_end: # Проверяем, что есть исторические данные в этом диапазоне\n",
        "                    for i in range(self.K):\n",
        "                        axs[i].plot(range(history_prediction_start, history_prediction_end),\n",
        "                                    self.full_history[history_prediction_start:history_prediction_end, i],\n",
        "                                    label=f'Исходные данные (предсказание) (Переменная {i+1})', color='green', alpha=0.6)\n",
        "\n",
        "        for i in range(self.K):\n",
        "            axs[i].set_title(f'Переменная {i+1}')\n",
        "            axs[i].set_ylabel('Значение')\n",
        "            axs[i].legend()\n",
        "            axs[i].grid(True)\n",
        "\n",
        "        plt.suptitle(f'Векторная авторегрессионная модель VAR({self.p}) с MLE (без константы)')\n",
        "        plt.xlabel('Время')\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uybD6_gb8PFj"
      },
      "outputs": [],
      "source": [
        "# --- Пример использования ---\n",
        "\n",
        "# 1. Создание экземпляра класса с заданным количеством переменных\n",
        "K_dim = 2\n",
        "var1_model = VAR1_MLE(K=K_dim)\n",
        "\n",
        "# Истинные параметры для генерации данных\n",
        "true_A = np.array([[0.6, 0.3],\n",
        "                    [-0.1, 0.5]])\n",
        "true_cov_matrix = np.array([[1.0, 0.5],\n",
        "                             [0.5, 2.0]])\n",
        "\n",
        "length = 200\n",
        "n_predict_example = 20 # Количество шагов для предсказания\n",
        "\n",
        "# Генерируем данные для новой модели\n",
        "sample_data = var1_model.sample(\n",
        "    A=true_A,\n",
        "    length=length,\n",
        "    Sigma=true_cov_matrix\n",
        ")\n",
        "\n",
        "# 2. Обучение модели (оценка A, и ковариации)\n",
        "var1_model.fit(data=sample_data, n_predict=n_predict_example)\n",
        "\n",
        "# 3. Построение графика\n",
        "var1_model.plot(confidence_level=0.95, n_predict=n_predict_example, start=150, end=220)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIxUXb9py-Sh"
      },
      "source": [
        "### Модель маятника\n",
        "\n",
        "Рассмотрим модель математического маятника (это модель малый колебаний, без нелинейных эффектов физического маятника).\n",
        "\n",
        "В непрерывном времени через законы Ньютона и предполагая $\\sin x \\approx x$ (так как малые колебания) мы можем получить такое уравнение:\n",
        "\n",
        "$$\n",
        "\\ddot{x}=-w^2 x,\n",
        "$$\n",
        "\n",
        "где две точки обозначают вторую производную, $x$ -- угловое отклонение маятника от положения равновесия (в радианах), $\\omega^2$ -- коэффициент, определяющий частоту колебаний. Это дифференциальное уравнение. Оно сводится к системе двух уравнений первого порядка вводом новой переменной $v$:\n",
        "\n",
        "$$\n",
        "\\dot{x} = v, \\quad \\dot{v} = - \\omega^2 x.\n",
        "$$\n",
        "\n",
        "Мы полагаем известными начальное положение $x(0)=x_0$ И начальную скорость $v(0)=v_0$.\n",
        "\n",
        "Если дискретизировать его по времени (например, методом Эйлера), то мы получим с учётом неопределённости ошибок метода (которые мы моделируем гауссовскими шумами) следующую систему уравнений:\n",
        "\n",
        "<details>\n",
        "\n",
        "<summary>Спойлер</summary>\n",
        "\n",
        "$$ \\left\\{\\begin{array}{l} x_{n+1}=x_{n} + v_n \\Delta t+\\varepsilon_{n+1}, \\quad \\varepsilon_{n+1} \\sim \\mathcal{N}\\left(0, {\\sigma_{x}}^2\\right) \\\\ v_{n+1}=v_n -\\omega^2 x_n \\Delta t+\\eta_{n+1}, \\quad \\eta_{n+1} \\sim \\mathcal{N}(0, {\\sigma_{v}}^2) \\end{array}\\right.$$\n",
        "\n",
        "Если мы наблюдаем $x_i,~v_i$, то по этим данным мы можем попробовать оценить параметры маятника. Вообще это в точности модель $VAR(1)$ со своей специфичной структурой:\n",
        "\n",
        "$$\n",
        "X_i = \\begin{bmatrix} x_i \\\\ v_i\\end{bmatrix}, \\quad A = \\begin{bmatrix} 1 & \\Delta t\\\\ -\\omega^2 \\Delta t & 1\\end{bmatrix}, \\quad \\Sigma = \\begin{bmatrix} \\sigma^2_x & 0\\\\0 & \\sigma^2_v\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "</details>\n",
        "\n",
        "Мы попробуем её оценить с помощью общего класса $VAR(1)$ и с использованием знания о структуре.\n",
        "\n",
        "Видно, что численный метод несовершенен, потому что закон сохранения энергии не выполняется. Как это можно заметить из матрицы $A$?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhHDF43LT6D_"
      },
      "outputs": [],
      "source": [
        "# --- Пример использования VAR(1)---\n",
        "# ༼ つ ◕_◕ ༽つ\n",
        "# Попробуйте!\n",
        "# Если трудно, можете свериться с заполненной версией"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyEF9DtSMbPd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
